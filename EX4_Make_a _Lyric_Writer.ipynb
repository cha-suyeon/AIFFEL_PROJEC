{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "wanted-shopper",
   "metadata": {},
   "source": [
    "# Exploration 4\n",
    "## Project: Make a nice lyric writer!\n",
    "### 5 Steps in the project\n",
    "1. 데이터 다운로드\n",
    "2. 데이터 읽어오기\n",
    "3. 데이터 정제\n",
    "- Standardize\n",
    "- Tokenize\n",
    "- Vectorization\n",
    "4. 평가 데이터셋 분리\n",
    "5. 모델로 학습하기\n",
    "6. 프로젝트 정리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-minimum",
   "metadata": {},
   "source": [
    "### 참고\n",
    "- [glob.glob(pathname, *, recursive=False)](https://docs.python.org/ko/3/library/glob.html) : 경로 지정을 포함하는 문자열인 pathname에 일치하는 경로 이름의 비어있을 수 있는 리스트를 반환합니다.\n",
    "- splitlines(): 줄바꿈 기호에 따라 하나의 리스트로 저장해주는 메소드 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breathing-dream",
   "metadata": {},
   "source": [
    "# 1. 데이터 다운로드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stunning-authentication",
   "metadata": {},
   "source": [
    "- mkdir -p ~/aiffel/lyricist/models\n",
    "- ln -s ~/data ~/aiffel/lyricist/data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aging-piano",
   "metadata": {},
   "source": [
    "# 2. 데이터 읽어오기\n",
    "- 사용할 라이브러리불러오기\n",
    "- 파일 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "mighty-merchandise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples: \n",
      " ['At first I was afraid', 'I was petrified', 'I kept thinking I could never live without you', 'By my side But then I spent so many nights', \"Just thinking how you've done me wrong\"]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import shutil\n",
    "import string\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담기\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples: \\n\", raw_corpus[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "introductory-template",
   "metadata": {},
   "source": [
    "- 오늘의 Dataset은 Lyrics, 노래 가사입니다.\n",
    "- TMI👀: Examples로 나온 가사는 Gloria Gaynor의 '[I will survive](https://youtu.be/ARt9HV9T0w8)'네요. 궁금하면 제목을 클릭해서 들어보세요! 듣자마자 다들 \"아! 이 노래~😆\" 하실 겁니다.\n",
    "\n",
    "#### raw_corpus 로 데이터를 더 자세히 보도록 하겠습니다! (길어서 삭제)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "certain-groove",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_corpus\n",
    "# 풀어서 실행해보세요!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separate-export",
   "metadata": {},
   "source": [
    "훑어보면서 특수 문자나 불필요한 기호가 있어보이는지 살펴봅니다.\n",
    "\n",
    "- '!(exclamation mark)', '?(question mark)'가 자주 보입니다.\n",
    "- 문장 속 연결 부분에  ',(comma)' 도 많이 보이네요. ex) 'Oh no, oh no, no, no Okay, all right, oh no'\n",
    "- '( )' 괄호 안에 들어간 것도 가사이니 괄호도 삭제해주면 좋을 것 같습니다.\n",
    "- 가끔 '-(hyphen)'도 보입니다. ex) 'a-wigglin'\n",
    "\n",
    "#### 이제 데이터 정제를 시작합니다.\n",
    "\n",
    "- 여기서 찾아둔 필요 없는 기호 등을 삭제하고, 필요한 문장과 필요 없는 문장을 구분하여 삭제 또는 수정을 통해 데이터 정제를 시작합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "another-sculpture",
   "metadata": {},
   "source": [
    "# 3. 데이터 정제\n",
    "## 3.(1) 표준화 (Standardize)\n",
    "- 표준화는 일반적으로 데이터 세트를 단순화하기 위해 구두점이나 HTML 요소를 제거하기 위해 텍스트를 전처리하는 것을 말합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "covered-teach",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At first I was afraid\n",
      "I was petrified\n",
      "I kept thinking I could never live without you\n",
      "By my side But then I spent so many nights\n",
      "Just thinking how you've done me wrong\n",
      "I grew strong\n",
      "I learned how to get along And so you're back\n",
      "From outer space\n",
      "I just walked in to find you\n",
      "Here without that look upon your face I should have changed that fucking lock\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue\n",
    "        # 길이가 0인 문장 삭제\n",
    "\n",
    "    if idx > 9: break\n",
    "        # 일단 문장 10개만 확인해보기\n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prime-arlington",
   "metadata": {},
   "source": [
    "#### 아래 코드 데이터 정제 순서\n",
    "\n",
    "1. 소문자로 바꾸고, 양쪽 공백을 지웁니다\n",
    "2. 특수문자 양쪽에 공백을 넣고\n",
    "3. 여러개의 공백은 하나의 공백으로 바꿉니다\n",
    "4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿉니다\n",
    "5. 다시 양쪽 공백을 지웁니다\n",
    "6. 문장 시작에는 '<'start'>', 끝에는 '<'end'>'를 추가합니다\n",
    "- 이 순서로 직접 텍스트를 표준화 해주는 함수를 작성해 봅시다! 위의 순서대로 처리해주면 문제가 되는 상황을 방지할 수 있겠네요! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "spread-expansion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 1\n",
    "    sentence = re.sub(r\"([-?.!,¿])\", r\" \\1 \", sentence) # 2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n",
    "    sentence = sentence.strip() # 5\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 6\n",
    "    return sentence\n",
    "\n",
    "# 문장 하나를 필터링한 예시 확인\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acceptable-divorce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> at first i was afraid <end>',\n",
       " '<start> i was petrified <end>',\n",
       " '<start> i kept thinking i could never live without you <end>',\n",
       " '<start> by my side but then i spent so many nights <end>',\n",
       " '<start> just thinking how you ve done me wrong <end>',\n",
       " '<start> i grew strong <end>',\n",
       " '<start> i learned how to get along and so you re back <end>',\n",
       " '<start> from outer space <end>',\n",
       " '<start> i just walked in to find you <end>',\n",
       " '<start> here without that look upon your face i should have changed that fucking lock <end>']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 여기에 정제된 문장을 모을겁니다\n",
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    # 우리가 원하지 않는 문장은 건너뜁니다\n",
    "    if len(sentence) == 0: continue\n",
    "    \n",
    "    # 정제를 하고 담아주세요\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "        \n",
    "# 정제된 결과를 10개만 확인해보죠\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "applied-fleece",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175986"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southern-package",
   "metadata": {},
   "source": [
    "- corpus에 담은 정제된 데이터가 187088개에서 175986개로 줄었음을 알 수 있네요!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collectible-ambassador",
   "metadata": {},
   "source": [
    "## 3.(2) 토큰화 (Tokenize)\n",
    "-  문자열을 토큰으로 분할하는 것을 말합니다(예: 공백에서 분할하여 문장을 개별 단어로 분할)\n",
    "\n",
    "## 3. (3) 벡터화 (Vectorization)\n",
    "- 벡터화는 토큰을 신경망에 제공할 수 있도록 숫자로 변환하는 것을 말합니다.\n",
    "\n",
    "\n",
    "### 설참📜 토큰화 진행 전에 보면 좋은 TensorFlow 자료! \n",
    "- 📌[tf.keras.preprocessing.text.Tokenizer](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer)\n",
    "\n",
    "- 📌[tf.keras.preprocessing.sequence.pad_sequences](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences)\n",
    "- 📌[tf.data.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "vietnamese-destruction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   71  241 ...    0    0    0]\n",
      " [   2    5   57 ...    0    0    0]\n",
      " [   2    5 1087 ...    0    0    0]\n",
      " ...\n",
      " [   2   48   16 ...    0    0    0]\n",
      " [  25    9 2859 ...  264   19    3]\n",
      " [   2    6  181 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f42dcdcf990>\n"
     ]
    }
   ],
   "source": [
    "# 토큰화 할 때 텐서플로우의 Tokenizer와 pad_sequences를 사용\n",
    "\n",
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000, \n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    \n",
    "    # corpus를 이용해 tokenizer 내부의 단어장을 완성\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    \n",
    "    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    \n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞춤\n",
    "    # 만약 시퀀스가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞춤\n",
    "    # 문장 앞에 패딩을 붙여 길이를 맞추고 싶다면 padding='pre'를 사용\n",
    "    \n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=15)   \n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "duplicate-fetish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   71  241    5   57  665    3    0    0    0]\n",
      " [   2    5   57 6490    3    0    0    0    0    0]\n",
      " [   2    5 1087  533    5  103   79  205  258    7]]\n"
     ]
    }
   ],
   "source": [
    "# 텐서 데이터 조회\n",
    "\n",
    "print(tensor[:3, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fantastic-cheat",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : i\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "# 단어 사전 조회\n",
    "\n",
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "congressional-publisher",
   "metadata": {},
   "source": [
    "- tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성\n",
    "- 마지막 토큰은 '<'end'>'가 아니라 '<'pad'>'일 가능성이 높음\n",
    "- num_words를 12,000개로 맞췄기 때문에 토큰이 12,000개 보다 적은 경우에는 마지막이 pad로 끝나서 가능성이 높다고 볼 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spatial-shareware",
   "metadata": {},
   "source": [
    "## Source, Target 으로 구분한 Dataset 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "focal-karen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2  71 241   5  57 665   3   0   0   0   0   0   0   0]\n",
      "[ 71 241   5  57 665   3   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1]  \n",
    "\n",
    "# tensor에서 <start>를 잘라내서 타겟 문장을 생성\n",
    "tgt_input = tensor[:, 1:]    \n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "athletic-mineral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Tenser : (175986, 15)\n",
      "Shape of Source : (175986, 14)\n",
      "Shape of Target : (175986, 14)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of Tenser :', tensor.shape)\n",
    "print('Shape of Source :', src_input.shape)\n",
    "print('Shape of Target :', tgt_input.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-advancement",
   "metadata": {},
   "source": [
    "- 모든 토큰이 텐서로 바뀐 것을 볼 수 있습니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sound-binding",
   "metadata": {},
   "source": [
    "# 4. 훈련/평가 데이터셋 분리\n",
    "\n",
    "- tokenize() 함수로 데이터를 Tensor로 변환하였으니, sklearn 모듈의 train_test_split() 함수를 사용해 훈련 데이터와 평가 데이터를 분리하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "chemical-objective",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input,\n",
    "                                                          tgt_input,\n",
    "                                                          test_size=0.2,\n",
    "                                                          random_state=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "offshore-background",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (140788, 14)\n",
      "Target Train: (140788, 14)\n",
      "Source Test : (35198, 14)\n",
      "Target Test : (35198, 14)\n"
     ]
    }
   ],
   "source": [
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)\n",
    "print('Source Test :', enc_val.shape)\n",
    "print('Target Test :', dec_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "sealed-authority",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "matched-characterization",
   "metadata": {},
   "source": [
    "지금까지의 과정을 다시 한 번 정리해보면 기까지 해온 과정을 다시 한 번 정리해보면\n",
    "1. 정규표현식을 이용한 corpus 생성\n",
    "2. tf.keras.preprocessing.text.Tokenizer를 이용해 corpus를 텐서로 변환\n",
    "3. tf.data.Dataset.from_tensor_slices()를 이용해 corpus 텐서를 tf.data.Dataset객체로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vertical-restriction",
   "metadata": {},
   "source": [
    "## 5. 모델로 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "polish-purchase",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 14, 12001), dtype=float32, numpy=\n",
       "array([[[ 2.33203580e-04, -1.86599616e-04,  2.18321511e-04, ...,\n",
       "         -1.53840359e-04,  9.91375855e-05, -2.03850475e-04],\n",
       "        [ 2.64370115e-04, -2.77696119e-04,  4.84785269e-04, ...,\n",
       "         -1.53165965e-04, -1.92066873e-04, -3.30028706e-04],\n",
       "        [ 2.64633854e-04, -1.82564065e-04,  1.07968121e-03, ...,\n",
       "         -1.84216202e-04, -4.24272526e-04, -1.33159425e-04],\n",
       "        ...,\n",
       "        [ 2.66692281e-04, -4.34181129e-04,  7.11414381e-04, ...,\n",
       "          6.34632888e-04, -4.73407796e-04,  1.06723257e-03],\n",
       "        [ 2.30330828e-04, -4.13973961e-04,  4.17247385e-04, ...,\n",
       "          6.72830793e-04, -4.29097534e-04,  9.87319043e-04],\n",
       "        [ 1.99396527e-04, -4.05011931e-04,  1.50480744e-04, ...,\n",
       "          6.77563075e-04, -4.16178082e-04,  8.68083036e-04]],\n",
       "\n",
       "       [[ 2.33203580e-04, -1.86599616e-04,  2.18321511e-04, ...,\n",
       "         -1.53840359e-04,  9.91375855e-05, -2.03850475e-04],\n",
       "        [ 4.69908933e-04, -2.05484961e-04,  7.32813860e-05, ...,\n",
       "         -5.35549363e-04,  3.29091738e-04, -5.37954969e-04],\n",
       "        [ 8.26655480e-04, -1.35026392e-04,  9.19894737e-05, ...,\n",
       "         -9.48038301e-04,  2.97334627e-04, -5.46710275e-04],\n",
       "        ...,\n",
       "        [ 5.80134452e-04, -1.13094330e-03, -6.74890296e-04, ...,\n",
       "         -9.14470729e-05, -2.20799251e-04, -1.95385059e-04],\n",
       "        [ 3.95127339e-04, -1.08831923e-03, -7.70279788e-04, ...,\n",
       "          2.56301519e-05, -2.57829990e-04, -1.41338722e-04],\n",
       "        [ 2.51595513e-04, -1.03522663e-03, -8.59931228e-04, ...,\n",
       "          9.68105014e-05, -2.96059210e-04, -1.23809223e-04]],\n",
       "\n",
       "       [[ 2.33203580e-04, -1.86599616e-04,  2.18321511e-04, ...,\n",
       "         -1.53840359e-04,  9.91375855e-05, -2.03850475e-04],\n",
       "        [ 4.55302099e-04, -2.84294394e-04,  1.36263450e-04, ...,\n",
       "         -2.16125103e-04,  5.61685965e-06, -3.74614727e-04],\n",
       "        [ 4.54917812e-04, -2.26959499e-04,  7.85452212e-05, ...,\n",
       "         -2.25143580e-04, -1.51660817e-04, -3.16551042e-04],\n",
       "        ...,\n",
       "        [ 9.64142266e-04,  1.89466838e-04,  3.96285235e-04, ...,\n",
       "         -8.42880458e-04,  7.24957499e-04, -4.11719724e-04],\n",
       "        [ 9.43316380e-04,  2.64521586e-05,  4.37497336e-04, ...,\n",
       "         -6.09445735e-04,  7.01355806e-04, -9.30371636e-04],\n",
       "        [ 8.44484312e-04, -2.91269589e-05,  4.29587846e-04, ...,\n",
       "         -4.64104582e-04,  7.14920985e-04, -1.11246400e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 2.33203580e-04, -1.86599616e-04,  2.18321511e-04, ...,\n",
       "         -1.53840359e-04,  9.91375855e-05, -2.03850475e-04],\n",
       "        [ 1.19879689e-04, -3.99233104e-04,  4.08540334e-04, ...,\n",
       "         -3.14929086e-04,  1.39570242e-04,  1.29692446e-04],\n",
       "        [ 8.16765605e-05, -2.38920999e-04,  5.73544938e-04, ...,\n",
       "         -2.80770008e-04,  1.43001598e-04,  5.51339879e-04],\n",
       "        ...,\n",
       "        [ 7.01821875e-04, -2.80319247e-04, -5.78869833e-04, ...,\n",
       "          2.65650131e-04,  5.84669760e-04,  9.16881429e-04],\n",
       "        [ 5.55928738e-04, -3.09052644e-04, -7.52592052e-04, ...,\n",
       "          3.48367146e-04,  4.80611343e-04,  8.49335513e-04],\n",
       "        [ 4.29060368e-04, -3.33313394e-04, -9.12266260e-04, ...,\n",
       "          3.90218425e-04,  3.69164743e-04,  7.37705384e-04]],\n",
       "\n",
       "       [[ 2.33203580e-04, -1.86599616e-04,  2.18321511e-04, ...,\n",
       "         -1.53840359e-04,  9.91375855e-05, -2.03850475e-04],\n",
       "        [ 5.80378866e-04, -5.86664304e-04,  1.89936618e-04, ...,\n",
       "         -3.35501769e-04, -2.01421517e-05, -3.39710532e-04],\n",
       "        [ 9.73246701e-04, -8.88708979e-04,  4.79977189e-07, ...,\n",
       "         -2.40181471e-04,  5.94747253e-05, -5.20435628e-04],\n",
       "        ...,\n",
       "        [ 6.67466607e-04,  8.13224353e-04,  6.22006337e-05, ...,\n",
       "         -4.91939893e-04,  1.04456698e-03,  7.11769448e-04],\n",
       "        [ 5.14046871e-04,  7.58193666e-04,  4.16471667e-05, ...,\n",
       "         -3.32031224e-04,  9.26596869e-04,  7.31699984e-04],\n",
       "        [ 3.60508915e-04,  6.61942293e-04, -7.01030149e-05, ...,\n",
       "         -1.98717491e-04,  8.10282421e-04,  7.23663077e-04]],\n",
       "\n",
       "       [[ 2.33203580e-04, -1.86599616e-04,  2.18321511e-04, ...,\n",
       "         -1.53840359e-04,  9.91375855e-05, -2.03850475e-04],\n",
       "        [ 4.81884694e-04, -1.66976650e-04,  4.89050988e-04, ...,\n",
       "         -4.32820780e-05,  1.86745441e-04, -2.80512293e-04],\n",
       "        [ 6.24550157e-04, -4.74819273e-04,  5.19602618e-04, ...,\n",
       "          1.77859547e-04,  2.93622521e-04, -2.91126489e-04],\n",
       "        ...,\n",
       "        [-7.36999558e-04, -6.15697878e-04, -1.60607553e-04, ...,\n",
       "          1.94513792e-04, -4.53839253e-04,  3.49246489e-04],\n",
       "        [-8.53109465e-04, -5.23993280e-04, -1.03405364e-04, ...,\n",
       "          2.32831808e-04, -6.83820923e-04,  6.33495001e-05],\n",
       "        [-7.56503374e-04, -6.46647241e-04,  3.41932209e-05, ...,\n",
       "          5.05895820e-04, -8.86301568e-04, -2.72002915e-04]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)\n",
    "\n",
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "remarkable-merchandise",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  3072256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  12301025  \n",
      "=================================================================\n",
      "Total params: 29,012,961\n",
      "Trainable params: 29,012,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nasty-shelf",
   "metadata": {},
   "source": [
    "- 현재 만든 신경망 모델은 1개의 Embedding 레이어, 2개의 LSTM 레이어, 1개의 Dense 레이어로 구성되어 있습니다.\n",
    "- Embedding 레이어는 정수로 인코딩된 리뷰를 가져와 각 단어 인덱스에 대한 임베딩 벡터를 찾습니다. 이러한 벡터는 모델이 학습될 때 학습됩니다. 벡터는 출력 배열에 차원을 추가합니다. 결과 치수는 다음과 같습니다 (batch, sequence, embedding)\n",
    "- LSTM은 자신에게 입력된 시퀀스의 길이만큼 동일한 길이의 시퀀스를 출력한다는 의미입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "acting-flashing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "687/687 [==============================] - 245s 352ms/step - loss: 4.1124\n",
      "Epoch 2/30\n",
      "687/687 [==============================] - 244s 355ms/step - loss: 3.1728\n",
      "Epoch 3/30\n",
      "687/687 [==============================] - 244s 355ms/step - loss: 2.9579\n",
      "Epoch 4/30\n",
      "687/687 [==============================] - 244s 354ms/step - loss: 2.8031\n",
      "Epoch 5/30\n",
      "687/687 [==============================] - 243s 353ms/step - loss: 2.6699\n",
      "Epoch 6/30\n",
      "687/687 [==============================] - 243s 353ms/step - loss: 2.5539\n",
      "Epoch 7/30\n",
      "687/687 [==============================] - 243s 353ms/step - loss: 2.4443\n",
      "Epoch 8/30\n",
      "687/687 [==============================] - 243s 353ms/step - loss: 2.3385\n",
      "Epoch 9/30\n",
      "687/687 [==============================] - 243s 353ms/step - loss: 2.2400\n",
      "Epoch 10/30\n",
      "687/687 [==============================] - 242s 352ms/step - loss: 2.1530\n",
      "Epoch 11/30\n",
      "687/687 [==============================] - 242s 352ms/step - loss: 2.0638\n",
      "Epoch 12/30\n",
      "687/687 [==============================] - 242s 352ms/step - loss: 1.9842\n",
      "Epoch 13/30\n",
      "687/687 [==============================] - 242s 352ms/step - loss: 1.8988\n",
      "Epoch 14/30\n",
      "687/687 [==============================] - 241s 351ms/step - loss: 1.8195\n",
      "Epoch 15/30\n",
      "687/687 [==============================] - 242s 351ms/step - loss: 1.7422\n",
      "Epoch 16/30\n",
      "687/687 [==============================] - 243s 354ms/step - loss: 1.6695\n",
      "Epoch 17/30\n",
      "687/687 [==============================] - 244s 355ms/step - loss: 1.6028\n",
      "Epoch 18/30\n",
      "687/687 [==============================] - 244s 355ms/step - loss: 1.5368\n",
      "Epoch 19/30\n",
      "687/687 [==============================] - 244s 355ms/step - loss: 1.4731\n",
      "Epoch 20/30\n",
      "687/687 [==============================] - 244s 355ms/step - loss: 1.4139\n",
      "Epoch 21/30\n",
      "687/687 [==============================] - 244s 356ms/step - loss: 1.3578\n",
      "Epoch 22/30\n",
      "687/687 [==============================] - 244s 355ms/step - loss: 1.3042\n",
      "Epoch 23/30\n",
      "687/687 [==============================] - 244s 355ms/step - loss: 1.2558\n",
      "Epoch 24/30\n",
      "687/687 [==============================] - 244s 355ms/step - loss: 1.2127\n",
      "Epoch 25/30\n",
      "687/687 [==============================] - 244s 355ms/step - loss: 1.1731\n",
      "Epoch 26/30\n",
      "687/687 [==============================] - 244s 355ms/step - loss: 1.1350\n",
      "Epoch 27/30\n",
      "687/687 [==============================] - 244s 355ms/step - loss: 1.1019\n",
      "Epoch 28/30\n",
      "687/687 [==============================] - 244s 355ms/step - loss: 1.0718\n",
      "Epoch 29/30\n",
      "687/687 [==============================] - 244s 354ms/step - loss: 1.0448\n",
      "Epoch 30/30\n",
      "687/687 [==============================] - 242s 352ms/step - loss: 1.0172\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fce6852d850>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graduate-hygiene",
   "metadata": {},
   "source": [
    "- epoch 30을 실행하는 동안 loss는 4.11에서 loss 1.0172으로 내려갔습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "mature-production",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 단어 하나씩 예측해 문장을 만듭니다\n",
    "    #    1. 입력받은 문장의 텐서를 입력합니다.\n",
    "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다.\n",
    "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다.\n",
    "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다.\n",
    "    while True:\n",
    "        # 1\n",
    "        predict = model(test_tensor) \n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3 \n",
    "        test_tensor = tf.concat([test_tensor,\n",
    "                                 tf.expand_dims(predict_word, axis=0)],\n",
    "                                axis=-1)\n",
    "        # 4\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "moderate-threat",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> he formulate flavors darn darn cant cant cant fired fired fired fired barred barred barred barred hypnotize scented heathen '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> he\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "textile-shipping",
   "metadata": {},
   "source": [
    "- 'he'로 시작하는 문장을 만들었더니 'he s a monster'이란 문장이 처음 나왔다가 뒤에는 조금 희안한 문장이 나왔습니다.(아마 오래 걸려 모델 학습을 안 시켜서 그런 것 같습니다.)\n",
    "- 문법적으로 조금 아쉬운 부분은 있지만, 그래도 문장이 나왔다는 사실에 되게 기쁩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tender-subscription",
   "metadata": {},
   "source": [
    "## 6. 프로젝트 정리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "departmental-multiple",
   "metadata": {},
   "source": [
    "이번 Exploration 04번째 프로젝트는 노래 가사가 담긴 Dataset을 통해 작사를 해주는 인공지능을 만들어 보는 프로젝트였습니다.\n",
    "\n",
    "- 원데이터로 약 187088 문장이 담겨 있는 데이터였습니다.\n",
    "\n",
    "프로젝트를 통해 배울 수 있었던 것은,\n",
    "\n",
    "1. text 데이터는 어떻게 데이터를 전처리 해야하는지?\n",
    "- 표준화, 토큰화, 벡터화의 기본 기능을 익힐 수 있었습니다.\n",
    "2. 훈련/평가 데이터를 분리하여 모델을 통해 학습하는 방법\n",
    "-  Sequential 모델 등 다른 model을 구성해보려고 했지만, 오류로 lms에서 기본적으로 구성해줬던 모델로 학습을 할 수밖에 없었습니다.\n",
    "3. epochs를 30으로 돌렸을 때 loss가 많이 낮아졌지만 10으로 했을 때는 loss가 2.2보다 높을 것으로 예상됩니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proper-creativity",
   "metadata": {},
   "source": [
    "\n",
    "프로젝트 후 개인적으로 추가 공부를 해보자!\n",
    "- [Guide to the Sequential model](https://keras.io/ko/getting-started/sequential-model-guide/)\n",
    "- [Text classification guide](https://developers.google.com/machine-learning/guides/text-classification/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
