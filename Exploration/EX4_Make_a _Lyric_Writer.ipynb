{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "wanted-shopper",
   "metadata": {},
   "source": [
    "# Exploration 4\n",
    "## Project: Make a nice lyric writer!\n",
    "### 5 Steps in the project\n",
    "1. ë°ì´í„° ë‹¤ìš´ë¡œë“œ\n",
    "2. ë°ì´í„° ì½ì–´ì˜¤ê¸°\n",
    "3. ë°ì´í„° ì •ì œ\n",
    "- Standardize\n",
    "- Tokenize\n",
    "- Vectorization\n",
    "4. í‰ê°€ ë°ì´í„°ì…‹ ë¶„ë¦¬\n",
    "5. ëª¨ë¸ë¡œ í•™ìŠµí•˜ê¸°\n",
    "6. í”„ë¡œì íŠ¸ ì •ë¦¬"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-minimum",
   "metadata": {},
   "source": [
    "### ì°¸ê³ \n",
    "- [glob.glob(pathname, *, recursive=False)](https://docs.python.org/ko/3/library/glob.html) : ê²½ë¡œ ì§€ì •ì„ í¬í•¨í•˜ëŠ” ë¬¸ìì—´ì¸ pathnameì— ì¼ì¹˜í•˜ëŠ” ê²½ë¡œ ì´ë¦„ì˜ ë¹„ì–´ìˆì„ ìˆ˜ ìˆëŠ” ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "- splitlines(): ì¤„ë°”ê¿ˆ ê¸°í˜¸ì— ë”°ë¼ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥í•´ì£¼ëŠ” ë©”ì†Œë“œ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breathing-dream",
   "metadata": {},
   "source": [
    "# 1. ë°ì´í„° ë‹¤ìš´ë¡œë“œ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stunning-authentication",
   "metadata": {},
   "source": [
    "- mkdir -p ~/aiffel/lyricist/models\n",
    "- ln -s ~/data ~/aiffel/lyricist/data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aging-piano",
   "metadata": {},
   "source": [
    "# 2. ë°ì´í„° ì½ì–´ì˜¤ê¸°\n",
    "- ì‚¬ìš©í•  ë¼ì´ë¸ŒëŸ¬ë¦¬ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "- íŒŒì¼ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "mighty-merchandise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„° í¬ê¸°: 187088\n",
      "Examples: \n",
      " ['At first I was afraid', 'I was petrified', 'I kept thinking I could never live without you', 'By my side But then I spent so many nights', \"Just thinking how you've done me wrong\"]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import shutil\n",
    "import string\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# ì—¬ëŸ¬ê°œì˜ txt íŒŒì¼ì„ ëª¨ë‘ ì½ì–´ì„œ raw_corpus ì— ë‹´ê¸°\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"ë°ì´í„° í¬ê¸°:\", len(raw_corpus))\n",
    "print(\"Examples: \\n\", raw_corpus[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "introductory-template",
   "metadata": {},
   "source": [
    "- ì˜¤ëŠ˜ì˜ Datasetì€ Lyrics, ë…¸ë˜ ê°€ì‚¬ì…ë‹ˆë‹¤.\n",
    "- TMIğŸ‘€: Examplesë¡œ ë‚˜ì˜¨ ê°€ì‚¬ëŠ” Gloria Gaynorì˜ '[I will survive](https://youtu.be/ARt9HV9T0w8)'ë„¤ìš”. ê¶ê¸ˆí•˜ë©´ ì œëª©ì„ í´ë¦­í•´ì„œ ë“¤ì–´ë³´ì„¸ìš”! ë“£ìë§ˆì ë‹¤ë“¤ \"ì•„! ì´ ë…¸ë˜~ğŸ˜†\" í•˜ì‹¤ ê²ë‹ˆë‹¤.\n",
    "\n",
    "#### raw_corpus ë¡œ ë°ì´í„°ë¥¼ ë” ìì„¸íˆ ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤! (ê¸¸ì–´ì„œ ì‚­ì œ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "certain-groove",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_corpus\n",
    "# í’€ì–´ì„œ ì‹¤í–‰í•´ë³´ì„¸ìš”!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separate-export",
   "metadata": {},
   "source": [
    "í›‘ì–´ë³´ë©´ì„œ íŠ¹ìˆ˜ ë¬¸ìë‚˜ ë¶ˆí•„ìš”í•œ ê¸°í˜¸ê°€ ìˆì–´ë³´ì´ëŠ”ì§€ ì‚´í´ë´…ë‹ˆë‹¤.\n",
    "\n",
    "- '!(exclamation mark)', '?(question mark)'ê°€ ìì£¼ ë³´ì…ë‹ˆë‹¤.\n",
    "- ë¬¸ì¥ ì† ì—°ê²° ë¶€ë¶„ì—  ',(comma)' ë„ ë§ì´ ë³´ì´ë„¤ìš”. ex) 'Oh no, oh no, no, no Okay, all right, oh no'\n",
    "- '( )' ê´„í˜¸ ì•ˆì— ë“¤ì–´ê°„ ê²ƒë„ ê°€ì‚¬ì´ë‹ˆ ê´„í˜¸ë„ ì‚­ì œí•´ì£¼ë©´ ì¢‹ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤.\n",
    "- ê°€ë” '-(hyphen)'ë„ ë³´ì…ë‹ˆë‹¤. ex) 'a-wigglin'\n",
    "\n",
    "#### ì´ì œ ë°ì´í„° ì •ì œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.\n",
    "\n",
    "- ì—¬ê¸°ì„œ ì°¾ì•„ë‘” í•„ìš” ì—†ëŠ” ê¸°í˜¸ ë“±ì„ ì‚­ì œí•˜ê³ , í•„ìš”í•œ ë¬¸ì¥ê³¼ í•„ìš” ì—†ëŠ” ë¬¸ì¥ì„ êµ¬ë¶„í•˜ì—¬ ì‚­ì œ ë˜ëŠ” ìˆ˜ì •ì„ í†µí•´ ë°ì´í„° ì •ì œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "another-sculpture",
   "metadata": {},
   "source": [
    "# 3. ë°ì´í„° ì •ì œ\n",
    "## 3.(1) í‘œì¤€í™” (Standardize)\n",
    "- í‘œì¤€í™”ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ë°ì´í„° ì„¸íŠ¸ë¥¼ ë‹¨ìˆœí™”í•˜ê¸° ìœ„í•´ êµ¬ë‘ì ì´ë‚˜ HTML ìš”ì†Œë¥¼ ì œê±°í•˜ê¸° ìœ„í•´ í…ìŠ¤íŠ¸ë¥¼ ì „ì²˜ë¦¬í•˜ëŠ” ê²ƒì„ ë§í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "covered-teach",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At first I was afraid\n",
      "I was petrified\n",
      "I kept thinking I could never live without you\n",
      "By my side But then I spent so many nights\n",
      "Just thinking how you've done me wrong\n",
      "I grew strong\n",
      "I learned how to get along And so you're back\n",
      "From outer space\n",
      "I just walked in to find you\n",
      "Here without that look upon your face I should have changed that fucking lock\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue\n",
    "        # ê¸¸ì´ê°€ 0ì¸ ë¬¸ì¥ ì‚­ì œ\n",
    "\n",
    "    if idx > 9: break\n",
    "        # ì¼ë‹¨ ë¬¸ì¥ 10ê°œë§Œ í™•ì¸í•´ë³´ê¸°\n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prime-arlington",
   "metadata": {},
   "source": [
    "#### ì•„ë˜ ì½”ë“œ ë°ì´í„° ì •ì œ ìˆœì„œ\n",
    "\n",
    "1. ì†Œë¬¸ìë¡œ ë°”ê¾¸ê³ , ì–‘ìª½ ê³µë°±ì„ ì§€ì›ë‹ˆë‹¤\n",
    "2. íŠ¹ìˆ˜ë¬¸ì ì–‘ìª½ì— ê³µë°±ì„ ë„£ê³ \n",
    "3. ì—¬ëŸ¬ê°œì˜ ê³µë°±ì€ í•˜ë‚˜ì˜ ê³µë°±ìœ¼ë¡œ ë°”ê¿‰ë‹ˆë‹¤\n",
    "4. a-zA-Z?.!,Â¿ê°€ ì•„ë‹Œ ëª¨ë“  ë¬¸ìë¥¼ í•˜ë‚˜ì˜ ê³µë°±ìœ¼ë¡œ ë°”ê¿‰ë‹ˆë‹¤\n",
    "5. ë‹¤ì‹œ ì–‘ìª½ ê³µë°±ì„ ì§€ì›ë‹ˆë‹¤\n",
    "6. ë¬¸ì¥ ì‹œì‘ì—ëŠ” '<'start'>', ëì—ëŠ” '<'end'>'ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤\n",
    "- ì´ ìˆœì„œë¡œ ì§ì ‘ í…ìŠ¤íŠ¸ë¥¼ í‘œì¤€í™” í•´ì£¼ëŠ” í•¨ìˆ˜ë¥¼ ì‘ì„±í•´ ë´…ì‹œë‹¤! ìœ„ì˜ ìˆœì„œëŒ€ë¡œ ì²˜ë¦¬í•´ì£¼ë©´ ë¬¸ì œê°€ ë˜ëŠ” ìƒí™©ì„ ë°©ì§€í•  ìˆ˜ ìˆê² ë„¤ìš”! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "spread-expansion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 1\n",
    "    sentence = re.sub(r\"([-?.!,Â¿])\", r\" \\1 \", sentence) # 2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,Â¿]+\", \" \", sentence) # 4\n",
    "    sentence = sentence.strip() # 5\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 6\n",
    "    return sentence\n",
    "\n",
    "# ë¬¸ì¥ í•˜ë‚˜ë¥¼ í•„í„°ë§í•œ ì˜ˆì‹œ í™•ì¸\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acceptable-divorce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> at first i was afraid <end>',\n",
       " '<start> i was petrified <end>',\n",
       " '<start> i kept thinking i could never live without you <end>',\n",
       " '<start> by my side but then i spent so many nights <end>',\n",
       " '<start> just thinking how you ve done me wrong <end>',\n",
       " '<start> i grew strong <end>',\n",
       " '<start> i learned how to get along and so you re back <end>',\n",
       " '<start> from outer space <end>',\n",
       " '<start> i just walked in to find you <end>',\n",
       " '<start> here without that look upon your face i should have changed that fucking lock <end>']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ì—¬ê¸°ì— ì •ì œëœ ë¬¸ì¥ì„ ëª¨ì„ê²ë‹ˆë‹¤\n",
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    # ìš°ë¦¬ê°€ ì›í•˜ì§€ ì•ŠëŠ” ë¬¸ì¥ì€ ê±´ë„ˆëœë‹ˆë‹¤\n",
    "    if len(sentence) == 0: continue\n",
    "    \n",
    "    # ì •ì œë¥¼ í•˜ê³  ë‹´ì•„ì£¼ì„¸ìš”\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "        \n",
    "# ì •ì œëœ ê²°ê³¼ë¥¼ 10ê°œë§Œ í™•ì¸í•´ë³´ì£ \n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "applied-fleece",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175986"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southern-package",
   "metadata": {},
   "source": [
    "- corpusì— ë‹´ì€ ì •ì œëœ ë°ì´í„°ê°€ 187088ê°œì—ì„œ 175986ê°œë¡œ ì¤„ì—ˆìŒì„ ì•Œ ìˆ˜ ìˆë„¤ìš”!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collectible-ambassador",
   "metadata": {},
   "source": [
    "## 3.(2) í† í°í™” (Tokenize)\n",
    "-  ë¬¸ìì—´ì„ í† í°ìœ¼ë¡œ ë¶„í• í•˜ëŠ” ê²ƒì„ ë§í•©ë‹ˆë‹¤(ì˜ˆ: ê³µë°±ì—ì„œ ë¶„í• í•˜ì—¬ ë¬¸ì¥ì„ ê°œë³„ ë‹¨ì–´ë¡œ ë¶„í• )\n",
    "\n",
    "## 3. (3) ë²¡í„°í™” (Vectorization)\n",
    "- ë²¡í„°í™”ëŠ” í† í°ì„ ì‹ ê²½ë§ì— ì œê³µí•  ìˆ˜ ìˆë„ë¡ ìˆ«ìë¡œ ë³€í™˜í•˜ëŠ” ê²ƒì„ ë§í•©ë‹ˆë‹¤.\n",
    "\n",
    "\n",
    "### ì„¤ì°¸ğŸ“œ í† í°í™” ì§„í–‰ ì „ì— ë³´ë©´ ì¢‹ì€ TensorFlow ìë£Œ! \n",
    "- ğŸ“Œ[tf.keras.preprocessing.text.Tokenizer](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer)\n",
    "\n",
    "- ğŸ“Œ[tf.keras.preprocessing.sequence.pad_sequences](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences)\n",
    "- ğŸ“Œ[tf.data.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "vietnamese-destruction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   71  241 ...    0    0    0]\n",
      " [   2    5   57 ...    0    0    0]\n",
      " [   2    5 1087 ...    0    0    0]\n",
      " ...\n",
      " [   2   48   16 ...    0    0    0]\n",
      " [  25    9 2859 ...  264   19    3]\n",
      " [   2    6  181 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f42dcdcf990>\n"
     ]
    }
   ],
   "source": [
    "# í† í°í™” í•  ë•Œ í…ì„œí”Œë¡œìš°ì˜ Tokenizerì™€ pad_sequencesë¥¼ ì‚¬ìš©\n",
    "\n",
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000, \n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    \n",
    "    # corpusë¥¼ ì´ìš©í•´ tokenizer ë‚´ë¶€ì˜ ë‹¨ì–´ì¥ì„ ì™„ì„±\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    \n",
    "    # ì¤€ë¹„í•œ tokenizerë¥¼ ì´ìš©í•´ corpusë¥¼ Tensorë¡œ ë³€í™˜\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    \n",
    "    # ì…ë ¥ ë°ì´í„°ì˜ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ì¼ì •í•˜ê²Œ ë§ì¶¤\n",
    "    # ë§Œì•½ ì‹œí€€ìŠ¤ê°€ ì§§ë‹¤ë©´ ë¬¸ì¥ ë’¤ì— íŒ¨ë”©ì„ ë¶™ì—¬ ê¸¸ì´ë¥¼ ë§ì¶¤\n",
    "    # ë¬¸ì¥ ì•ì— íŒ¨ë”©ì„ ë¶™ì—¬ ê¸¸ì´ë¥¼ ë§ì¶”ê³  ì‹¶ë‹¤ë©´ padding='pre'ë¥¼ ì‚¬ìš©\n",
    "    \n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=15)   \n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "duplicate-fetish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   71  241    5   57  665    3    0    0    0]\n",
      " [   2    5   57 6490    3    0    0    0    0    0]\n",
      " [   2    5 1087  533    5  103   79  205  258    7]]\n"
     ]
    }
   ],
   "source": [
    "# í…ì„œ ë°ì´í„° ì¡°íšŒ\n",
    "\n",
    "print(tensor[:3, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fantastic-cheat",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : i\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "# ë‹¨ì–´ ì‚¬ì „ ì¡°íšŒ\n",
    "\n",
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "congressional-publisher",
   "metadata": {},
   "source": [
    "- tensorì—ì„œ ë§ˆì§€ë§‰ í† í°ì„ ì˜ë¼ë‚´ì„œ ì†ŒìŠ¤ ë¬¸ì¥ì„ ìƒì„±\n",
    "- ë§ˆì§€ë§‰ í† í°ì€ '<'end'>'ê°€ ì•„ë‹ˆë¼ '<'pad'>'ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŒ\n",
    "- num_wordsë¥¼ 12,000ê°œë¡œ ë§ì·„ê¸° ë•Œë¬¸ì— í† í°ì´ 12,000ê°œ ë³´ë‹¤ ì ì€ ê²½ìš°ì—ëŠ” ë§ˆì§€ë§‰ì´ padë¡œ ëë‚˜ì„œ ê°€ëŠ¥ì„±ì´ ë†’ë‹¤ê³  ë³¼ ìˆ˜ ìˆìŒ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spatial-shareware",
   "metadata": {},
   "source": [
    "## Source, Target ìœ¼ë¡œ êµ¬ë¶„í•œ Dataset ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "focal-karen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2  71 241   5  57 665   3   0   0   0   0   0   0   0]\n",
      "[ 71 241   5  57 665   3   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1]  \n",
    "\n",
    "# tensorì—ì„œ <start>ë¥¼ ì˜ë¼ë‚´ì„œ íƒ€ê²Ÿ ë¬¸ì¥ì„ ìƒì„±\n",
    "tgt_input = tensor[:, 1:]    \n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "athletic-mineral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Tenser : (175986, 15)\n",
      "Shape of Source : (175986, 14)\n",
      "Shape of Target : (175986, 14)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of Tenser :', tensor.shape)\n",
    "print('Shape of Source :', src_input.shape)\n",
    "print('Shape of Target :', tgt_input.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-advancement",
   "metadata": {},
   "source": [
    "- ëª¨ë“  í† í°ì´ í…ì„œë¡œ ë°”ë€ ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sound-binding",
   "metadata": {},
   "source": [
    "# 4. í›ˆë ¨/í‰ê°€ ë°ì´í„°ì…‹ ë¶„ë¦¬\n",
    "\n",
    "- tokenize() í•¨ìˆ˜ë¡œ ë°ì´í„°ë¥¼ Tensorë¡œ ë³€í™˜í•˜ì˜€ìœ¼ë‹ˆ, sklearn ëª¨ë“ˆì˜ train_test_split() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ í›ˆë ¨ ë°ì´í„°ì™€ í‰ê°€ ë°ì´í„°ë¥¼ ë¶„ë¦¬í•˜ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "chemical-objective",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input,\n",
    "                                                          tgt_input,\n",
    "                                                          test_size=0.2,\n",
    "                                                          random_state=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "offshore-background",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (140788, 14)\n",
      "Target Train: (140788, 14)\n",
      "Source Test : (35198, 14)\n",
      "Target Test : (35198, 14)\n"
     ]
    }
   ],
   "source": [
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)\n",
    "print('Source Test :', enc_val.shape)\n",
    "print('Target Test :', dec_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "sealed-authority",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "matched-characterization",
   "metadata": {},
   "source": [
    "ì§€ê¸ˆê¹Œì§€ì˜ ê³¼ì •ì„ ë‹¤ì‹œ í•œ ë²ˆ ì •ë¦¬í•´ë³´ë©´ ê¸°ê¹Œì§€ í•´ì˜¨ ê³¼ì •ì„ ë‹¤ì‹œ í•œ ë²ˆ ì •ë¦¬í•´ë³´ë©´\n",
    "1. ì •ê·œí‘œí˜„ì‹ì„ ì´ìš©í•œ corpus ìƒì„±\n",
    "2. tf.keras.preprocessing.text.Tokenizerë¥¼ ì´ìš©í•´ corpusë¥¼ í…ì„œë¡œ ë³€í™˜\n",
    "3. tf.data.Dataset.from_tensor_slices()ë¥¼ ì´ìš©í•´ corpus í…ì„œë¥¼ tf.data.Datasetê°ì²´ë¡œ ë³€í™˜"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vertical-restriction",
   "metadata": {},
   "source": [
    "## 5. ëª¨ë¸ë¡œ í•™ìŠµí•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "polish-purchase",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 14, 12001), dtype=float32, numpy=\n",
       "array([[[ 2.33203580e-04, -1.86599616e-04,  2.18321511e-04, ...,\n",
       "         -1.53840359e-04,  9.91375855e-05, -2.03850475e-04],\n",
       "        [ 2.64370115e-04, -2.77696119e-04,  4.84785269e-04, ...,\n",
       "         -1.53165965e-04, -1.92066873e-04, -3.30028706e-04],\n",
       "        [ 2.64633854e-04, -1.82564065e-04,  1.07968121e-03, ...,\n",
       "         -1.84216202e-04, -4.24272526e-04, -1.33159425e-04],\n",
       "        ...,\n",
       "        [ 2.66692281e-04, -4.34181129e-04,  7.11414381e-04, ...,\n",
       "          6.34632888e-04, -4.73407796e-04,  1.06723257e-03],\n",
       "        [ 2.30330828e-04, -4.13973961e-04,  4.17247385e-04, ...,\n",
       "          6.72830793e-04, -4.29097534e-04,  9.87319043e-04],\n",
       "        [ 1.99396527e-04, -4.05011931e-04,  1.50480744e-04, ...,\n",
       "          6.77563075e-04, -4.16178082e-04,  8.68083036e-04]],\n",
       "\n",
       "       [[ 2.33203580e-04, -1.86599616e-04,  2.18321511e-04, ...,\n",
       "         -1.53840359e-04,  9.91375855e-05, -2.03850475e-04],\n",
       "        [ 4.69908933e-04, -2.05484961e-04,  7.32813860e-05, ...,\n",
       "         -5.35549363e-04,  3.29091738e-04, -5.37954969e-04],\n",
       "        [ 8.26655480e-04, -1.35026392e-04,  9.19894737e-05, ...,\n",
       "         -9.48038301e-04,  2.97334627e-04, -5.46710275e-04],\n",
       "        ...,\n",
       "        [ 5.80134452e-04, -1.13094330e-03, -6.74890296e-04, ...,\n",
       "         -9.14470729e-05, -2.20799251e-04, -1.95385059e-04],\n",
       "        [ 3.95127339e-04, -1.08831923e-03, -7.70279788e-04, ...,\n",
       "          2.56301519e-05, -2.57829990e-04, -1.41338722e-04],\n",
       "        [ 2.51595513e-04, -1.03522663e-03, -8.59931228e-04, ...,\n",
       "          9.68105014e-05, -2.96059210e-04, -1.23809223e-04]],\n",
       "\n",
       "       [[ 2.33203580e-04, -1.86599616e-04,  2.18321511e-04, ...,\n",
       "         -1.53840359e-04,  9.91375855e-05, -2.03850475e-04],\n",
       "        [ 4.55302099e-04, -2.84294394e-04,  1.36263450e-04, ...,\n",
       "         -2.16125103e-04,  5.61685965e-06, -3.74614727e-04],\n",
       "        [ 4.54917812e-04, -2.26959499e-04,  7.85452212e-05, ...,\n",
       "         -2.25143580e-04, -1.51660817e-04, -3.16551042e-04],\n",
       "        ...,\n",
       "        [ 9.64142266e-04,  1.89466838e-04,  3.96285235e-04, ...,\n",
       "         -8.42880458e-04,  7.24957499e-04, -4.11719724e-04],\n",
       "        [ 9.43316380e-04,  2.64521586e-05,  4.37497336e-04, ...,\n",
       "         -6.09445735e-04,  7.01355806e-04, -9.30371636e-04],\n",
       "        [ 8.44484312e-04, -2.91269589e-05,  4.29587846e-04, ...,\n",
       "         -4.64104582e-04,  7.14920985e-04, -1.11246400e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 2.33203580e-04, -1.86599616e-04,  2.18321511e-04, ...,\n",
       "         -1.53840359e-04,  9.91375855e-05, -2.03850475e-04],\n",
       "        [ 1.19879689e-04, -3.99233104e-04,  4.08540334e-04, ...,\n",
       "         -3.14929086e-04,  1.39570242e-04,  1.29692446e-04],\n",
       "        [ 8.16765605e-05, -2.38920999e-04,  5.73544938e-04, ...,\n",
       "         -2.80770008e-04,  1.43001598e-04,  5.51339879e-04],\n",
       "        ...,\n",
       "        [ 7.01821875e-04, -2.80319247e-04, -5.78869833e-04, ...,\n",
       "          2.65650131e-04,  5.84669760e-04,  9.16881429e-04],\n",
       "        [ 5.55928738e-04, -3.09052644e-04, -7.52592052e-04, ...,\n",
       "          3.48367146e-04,  4.80611343e-04,  8.49335513e-04],\n",
       "        [ 4.29060368e-04, -3.33313394e-04, -9.12266260e-04, ...,\n",
       "          3.90218425e-04,  3.69164743e-04,  7.37705384e-04]],\n",
       "\n",
       "       [[ 2.33203580e-04, -1.86599616e-04,  2.18321511e-04, ...,\n",
       "         -1.53840359e-04,  9.91375855e-05, -2.03850475e-04],\n",
       "        [ 5.80378866e-04, -5.86664304e-04,  1.89936618e-04, ...,\n",
       "         -3.35501769e-04, -2.01421517e-05, -3.39710532e-04],\n",
       "        [ 9.73246701e-04, -8.88708979e-04,  4.79977189e-07, ...,\n",
       "         -2.40181471e-04,  5.94747253e-05, -5.20435628e-04],\n",
       "        ...,\n",
       "        [ 6.67466607e-04,  8.13224353e-04,  6.22006337e-05, ...,\n",
       "         -4.91939893e-04,  1.04456698e-03,  7.11769448e-04],\n",
       "        [ 5.14046871e-04,  7.58193666e-04,  4.16471667e-05, ...,\n",
       "         -3.32031224e-04,  9.26596869e-04,  7.31699984e-04],\n",
       "        [ 3.60508915e-04,  6.61942293e-04, -7.01030149e-05, ...,\n",
       "         -1.98717491e-04,  8.10282421e-04,  7.23663077e-04]],\n",
       "\n",
       "       [[ 2.33203580e-04, -1.86599616e-04,  2.18321511e-04, ...,\n",
       "         -1.53840359e-04,  9.91375855e-05, -2.03850475e-04],\n",
       "        [ 4.81884694e-04, -1.66976650e-04,  4.89050988e-04, ...,\n",
       "         -4.32820780e-05,  1.86745441e-04, -2.80512293e-04],\n",
       "        [ 6.24550157e-04, -4.74819273e-04,  5.19602618e-04, ...,\n",
       "          1.77859547e-04,  2.93622521e-04, -2.91126489e-04],\n",
       "        ...,\n",
       "        [-7.36999558e-04, -6.15697878e-04, -1.60607553e-04, ...,\n",
       "          1.94513792e-04, -4.53839253e-04,  3.49246489e-04],\n",
       "        [-8.53109465e-04, -5.23993280e-04, -1.03405364e-04, ...,\n",
       "          2.32831808e-04, -6.83820923e-04,  6.33495001e-05],\n",
       "        [-7.56503374e-04, -6.46647241e-04,  3.41932209e-05, ...,\n",
       "          5.05895820e-04, -8.86301568e-04, -2.72002915e-04]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)\n",
    "\n",
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "remarkable-merchandise",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  3072256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  12301025  \n",
      "=================================================================\n",
      "Total params: 29,012,961\n",
      "Trainable params: 29,012,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nasty-shelf",
   "metadata": {},
   "source": [
    "- í˜„ì¬ ë§Œë“  ì‹ ê²½ë§ ëª¨ë¸ì€ 1ê°œì˜ Embedding ë ˆì´ì–´, 2ê°œì˜ LSTM ë ˆì´ì–´, 1ê°œì˜ Dense ë ˆì´ì–´ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
    "- Embedding ë ˆì´ì–´ëŠ” ì •ìˆ˜ë¡œ ì¸ì½”ë”©ëœ ë¦¬ë·°ë¥¼ ê°€ì ¸ì™€ ê° ë‹¨ì–´ ì¸ë±ìŠ¤ì— ëŒ€í•œ ì„ë² ë”© ë²¡í„°ë¥¼ ì°¾ìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë²¡í„°ëŠ” ëª¨ë¸ì´ í•™ìŠµë  ë•Œ í•™ìŠµë©ë‹ˆë‹¤. ë²¡í„°ëŠ” ì¶œë ¥ ë°°ì—´ì— ì°¨ì›ì„ ì¶”ê°€í•©ë‹ˆë‹¤. ê²°ê³¼ ì¹˜ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤ (batch, sequence, embedding)\n",
    "- LSTMì€ ìì‹ ì—ê²Œ ì…ë ¥ëœ ì‹œí€€ìŠ¤ì˜ ê¸¸ì´ë§Œí¼ ë™ì¼í•œ ê¸¸ì´ì˜ ì‹œí€€ìŠ¤ë¥¼ ì¶œë ¥í•œë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "acting-flashing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "687/687 [==============================] - 245s 352ms/step - loss: 4.1124\n",
      "Epoch 2/30\n",
      "687/687 [==============================] - 244s 355ms/step - loss: 3.1728\n",
      "Epoch 3/30\n",
      "687/687 [==============================] - 244s 355ms/step - loss: 2.9579\n",
      "Epoch 4/30\n",
      "687/687 [==============================] - 244s 354ms/step - loss: 2.8031\n",
      "Epoch 5/30\n",
      "687/687 [==============================] - 243s 353ms/step - loss: 2.6699\n",
      "Epoch 6/30\n",
      "687/687 [==============================] - 243s 353ms/step - loss: 2.5539\n",
      "Epoch 7/30\n",
      "687/687 [==============================] - 243s 353ms/step - loss: 2.4443\n",
      "Epoch 8/30\n",
      "687/687 [==============================] - 243s 353ms/step - loss: 2.3385\n",
      "Epoch 9/30\n",
      "687/687 [==============================] - 243s 353ms/step - loss: 2.2400\n",
      "Epoch 10/30\n",
      "687/687 [==============================] - 242s 352ms/step - loss: 2.1530\n",
      "Epoch 11/30\n",
      "687/687 [==============================] - 242s 352ms/step - loss: 2.0638\n",
      "Epoch 12/30\n",
      "687/687 [==============================] - 242s 352ms/step - loss: 1.9842\n",
      "Epoch 13/30\n",
      "687/687 [==============================] - 242s 352ms/step - loss: 1.8988\n",
      "Epoch 14/30\n",
      "687/687 [==============================] - 241s 351ms/step - loss: 1.8195\n",
      "Epoch 15/30\n",
      "687/687 [==============================] - 242s 351ms/step - loss: 1.7422\n",
      "Epoch 16/30\n",
      "687/687 [==============================] - 243s 354ms/step - loss: 1.6695\n",
      "Epoch 17/30\n",
      "687/687 [==============================] - 244s 355ms/step - loss: 1.6028\n",
      "Epoch 18/30\n",
      "687/687 [==============================] - 244s 355ms/step - loss: 1.5368\n",
      "Epoch 19/30\n",
      "687/687 [==============================] - 244s 355ms/step - loss: 1.4731\n",
      "Epoch 20/30\n",
      "687/687 [==============================] - 244s 355ms/step - loss: 1.4139\n",
      "Epoch 21/30\n",
      "687/687 [==============================] - 244s 356ms/step - loss: 1.3578\n",
      "Epoch 22/30\n",
      "687/687 [==============================] - 244s 355ms/step - loss: 1.3042\n",
      "Epoch 23/30\n",
      "687/687 [==============================] - 244s 355ms/step - loss: 1.2558\n",
      "Epoch 24/30\n",
      "687/687 [==============================] - 244s 355ms/step - loss: 1.2127\n",
      "Epoch 25/30\n",
      "687/687 [==============================] - 244s 355ms/step - loss: 1.1731\n",
      "Epoch 26/30\n",
      "687/687 [==============================] - 244s 355ms/step - loss: 1.1350\n",
      "Epoch 27/30\n",
      "687/687 [==============================] - 244s 355ms/step - loss: 1.1019\n",
      "Epoch 28/30\n",
      "687/687 [==============================] - 244s 355ms/step - loss: 1.0718\n",
      "Epoch 29/30\n",
      "687/687 [==============================] - 244s 354ms/step - loss: 1.0448\n",
      "Epoch 30/30\n",
      "687/687 [==============================] - 242s 352ms/step - loss: 1.0172\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fce6852d850>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graduate-hygiene",
   "metadata": {},
   "source": [
    "- epoch 30ì„ ì‹¤í–‰í•˜ëŠ” ë™ì•ˆ lossëŠ” 4.11ì—ì„œ loss 1.0172ìœ¼ë¡œ ë‚´ë ¤ê°”ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "mature-production",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ì„œ ì…ë ¥ë°›ì€ init_sentenceë„ í…ì„œë¡œ ë³€í™˜\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # ë‹¨ì–´ í•˜ë‚˜ì”© ì˜ˆì¸¡í•´ ë¬¸ì¥ì„ ë§Œë“­ë‹ˆë‹¤\n",
    "    #    1. ì…ë ¥ë°›ì€ ë¬¸ì¥ì˜ í…ì„œë¥¼ ì…ë ¥í•©ë‹ˆë‹¤.\n",
    "    #    2. ì˜ˆì¸¡ëœ ê°’ ì¤‘ ê°€ì¥ ë†’ì€ í™•ë¥ ì¸ word indexë¥¼ ë½‘ì•„ëƒ…ë‹ˆë‹¤.\n",
    "    #    3. 2ì—ì„œ ì˜ˆì¸¡ëœ word indexë¥¼ ë¬¸ì¥ ë’¤ì— ë¶™ì…ë‹ˆë‹¤.\n",
    "    #    4. ëª¨ë¸ì´ <end>ë¥¼ ì˜ˆì¸¡í–ˆê±°ë‚˜, max_lenì— ë„ë‹¬í–ˆë‹¤ë©´ ë¬¸ì¥ ìƒì„±ì„ ë§ˆì¹©ë‹ˆë‹¤.\n",
    "    while True:\n",
    "        # 1\n",
    "        predict = model(test_tensor) \n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3 \n",
    "        test_tensor = tf.concat([test_tensor,\n",
    "                                 tf.expand_dims(predict_word, axis=0)],\n",
    "                                axis=-1)\n",
    "        # 4\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizerë¥¼ ì´ìš©í•´ word indexë¥¼ ë‹¨ì–´ë¡œ í•˜ë‚˜ì”© ë³€í™˜í•©ë‹ˆë‹¤ \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "moderate-threat",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> he formulate flavors darn darn cant cant cant fired fired fired fired barred barred barred barred hypnotize scented heathen '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> he\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "textile-shipping",
   "metadata": {},
   "source": [
    "- 'he'ë¡œ ì‹œì‘í•˜ëŠ” ë¬¸ì¥ì„ ë§Œë“¤ì—ˆë”ë‹ˆ 'he s a monster'ì´ë€ ë¬¸ì¥ì´ ì²˜ìŒ ë‚˜ì™”ë‹¤ê°€ ë’¤ì—ëŠ” ì¡°ê¸ˆ í¬ì•ˆí•œ ë¬¸ì¥ì´ ë‚˜ì™”ìŠµë‹ˆë‹¤.(ì•„ë§ˆ ì˜¤ë˜ ê±¸ë ¤ ëª¨ë¸ í•™ìŠµì„ ì•ˆ ì‹œì¼œì„œ ê·¸ëŸ° ê²ƒ ê°™ìŠµë‹ˆë‹¤.)\n",
    "- ë¬¸ë²•ì ìœ¼ë¡œ ì¡°ê¸ˆ ì•„ì‰¬ìš´ ë¶€ë¶„ì€ ìˆì§€ë§Œ, ê·¸ë˜ë„ ë¬¸ì¥ì´ ë‚˜ì™”ë‹¤ëŠ” ì‚¬ì‹¤ì— ë˜ê²Œ ê¸°ì©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tender-subscription",
   "metadata": {},
   "source": [
    "## 6. í”„ë¡œì íŠ¸ ì •ë¦¬"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "departmental-multiple",
   "metadata": {},
   "source": [
    "ì´ë²ˆ Exploration 04ë²ˆì§¸ í”„ë¡œì íŠ¸ëŠ” ë…¸ë˜ ê°€ì‚¬ê°€ ë‹´ê¸´ Datasetì„ í†µí•´ ì‘ì‚¬ë¥¼ í•´ì£¼ëŠ” ì¸ê³µì§€ëŠ¥ì„ ë§Œë“¤ì–´ ë³´ëŠ” í”„ë¡œì íŠ¸ì˜€ìŠµë‹ˆë‹¤.\n",
    "\n",
    "- ì›ë°ì´í„°ë¡œ ì•½ 187088 ë¬¸ì¥ì´ ë‹´ê²¨ ìˆëŠ” ë°ì´í„°ì˜€ìŠµë‹ˆë‹¤.\n",
    "\n",
    "í”„ë¡œì íŠ¸ë¥¼ í†µí•´ ë°°ìš¸ ìˆ˜ ìˆì—ˆë˜ ê²ƒì€,\n",
    "\n",
    "1. text ë°ì´í„°ëŠ” ì–´ë–»ê²Œ ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬ í•´ì•¼í•˜ëŠ”ì§€?\n",
    "- í‘œì¤€í™”, í† í°í™”, ë²¡í„°í™”ì˜ ê¸°ë³¸ ê¸°ëŠ¥ì„ ìµí ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.\n",
    "2. í›ˆë ¨/í‰ê°€ ë°ì´í„°ë¥¼ ë¶„ë¦¬í•˜ì—¬ ëª¨ë¸ì„ í†µí•´ í•™ìŠµí•˜ëŠ” ë°©ë²•\n",
    "-  Sequential ëª¨ë¸ ë“± ë‹¤ë¥¸ modelì„ êµ¬ì„±í•´ë³´ë ¤ê³  í–ˆì§€ë§Œ, ì˜¤ë¥˜ë¡œ lmsì—ì„œ ê¸°ë³¸ì ìœ¼ë¡œ êµ¬ì„±í•´ì¤¬ë˜ ëª¨ë¸ë¡œ í•™ìŠµì„ í•  ìˆ˜ë°–ì— ì—†ì—ˆìŠµë‹ˆë‹¤.\n",
    "3. epochsë¥¼ 30ìœ¼ë¡œ ëŒë ¸ì„ ë•Œ lossê°€ ë§ì´ ë‚®ì•„ì¡Œì§€ë§Œ 10ìœ¼ë¡œ í–ˆì„ ë•ŒëŠ” lossê°€ 2.2ë³´ë‹¤ ë†’ì„ ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proper-creativity",
   "metadata": {},
   "source": [
    "\n",
    "í”„ë¡œì íŠ¸ í›„ ê°œì¸ì ìœ¼ë¡œ ì¶”ê°€ ê³µë¶€ë¥¼ í•´ë³´ì!\n",
    "- [Guide to the Sequential model](https://keras.io/ko/getting-started/sequential-model-guide/)\n",
    "- [Text classification guide](https://developers.google.com/machine-learning/guides/text-classification/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
